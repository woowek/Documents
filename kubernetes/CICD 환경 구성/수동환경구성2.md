- 상세 스펙이 나왔다. 
    * k8s
    * gitea
    * jenkins
    * habor
    * argocd
    * keyclock


- IP 변경
    * /etc/sysconfig/network-scrips 수정
    * systemctl restart NetworkManager
    * 문제있으면 재부틸
    * BOOTPROTO=none 과 ONBOOT=yes 설정 확인
    * 이마저 안되면 ip a 명령어와 nmcli connection show ens192 | grep mac 명령어로 나오는 macaddress가 일치하는지 확인
    * 일치하지 않다면
    ```
    nmcli connection modify ens192 802-3-ethernet.mac-address ""
    nmcli connection modify ens192 802-3-ethernet.cloned-mac-address ""
    nmcli connection reload
    nmcli connection down ens192
    nmcli connection up ens192
    ```
    - 후 재부팅



- 일단 있던 k3s 지우고 k8s로 다시 깔자..
    * 삭제
    ```
    /usr/local/bin/k3s-uninstall.sh
    /usr/local/bin/k3s-agent-uninstall.sh
    ```
    * k8s 설치
        - swap off
            * swapoff -a
        - chrony 설치
            * yum install chrony
            * systemctl enable --now chronyd

        - 포트 오픈
            * firewall-cmd --permanent --zone=public --add-port={80,443,6443,2379,2380,10250,10251,10252,30000-32767}/tcp
            * firewall-cmd --reload
                - 마스터 노드에서 필요한 필수 포트
                    * 6443 포트 : Kubernetes API Server / Used By All
                    * 2379~2380 포트 : etcd server client API / Used By kube-apiserver, etcd
                    * 10250 포트 : Kubelet API / Used By Self, Control plane
                    * 10251 포트 : kube-scheduler / Used By Self
                    * 10252 포트 : kube-controller-manager / Used By Self
                - 워커 노드에서 필요한 필수 포트
                    * 10250 포트 : Kubelet API / Used By Self, Control plane
                    * 30000~32767 포트 : NodePort Services / Used By All -->


        - 설치 커멘드 실행
            ```
            cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
            [kubernetes]
            name=Kubernetes
            baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/
            enabled=1
            gpgcheck=1
            gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key
            exclude=kubelet kubeadm kubectl
            EOF

            sudo setenforce 0
            sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
            sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
            sudo systemctl enable --now kubelet
            ```
        - docker 설치
            ```
            for pkg in docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine podman-docker containerd.io runc; do sudo dnf remove -y $pkg; done
            ```          
            ```
            # Docker의 공식 GPG 키를 추가합니다:
            sudo dnf install -y yum-utils
            sudo yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo

            sudo mkdir -p /etc/pki/rpm-gpg
            curl -fsSL https://download.docker.com/linux/centos/gpg | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-docker > /dev/null

            # Docker 저장소를 추가합니다:
            sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

            # Rocky Linux 9에서는 CentOS 8의 저장소를 사용하므로, 아래 명령어로 변경해야 할 수도 있습니다.
            # sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
            # sudo sed -i 's/\$releasever/8/g' /etc/yum.repos.d/docker-ce.repo

            # 패키지 목록을 업데이트합니다:
            sudo dnf update -y
            ```
        - containerd 설치
            ```
            sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            # Containerd 설정
            sudo mkdir -p /etc/containerd
            containerd config default | sudo tee /etc/containerd/config.toml
            vi /etc/containerd/config.toml
            ..
            [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]
            SystemdCgroup = true
            ..
            systemctl status containerd
            systemctl enable --now containerd
            ```
        - calico 설치
            ```
            curl https://raw.githubusercontent.com/projectcalico/calico/v3.27.5/manifests/calico.yaml -O
            kubectl apply -f calico.yaml
            ```
        - kubeadm init - MasterNode
            <!-- * kubeadm init --pod-network-cidr=10.89.0.0/12
            * sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config -->
            * kubeadm init --apiserver-advertise-address=마스터노드IP --v=5
            * kubeadm init  --pod-network-cidr=10.50.0.0/12 => 자꾸 안되서 이걸로 해봄
            * 설치시 다음 에러가 나타난다면..
            ```
            [ERROR SystemVerification]: kernel release 4.18.0-553.34.1.el8_10.x86_64 is unsupported. Recommended LTS version from the 4.x series is 4.19. Any 5.x or 6.x versions are also supported. For cgroups v2 support, the minimal version is 4.15 and the recommended version is 5.8+
            ```
                - ELRepo 저장소 추가 명령어
                ```
                dnf install -y epel-release
                dnf install -y https://www.elrepo.org/elrepo-release-8.el8.elrepo.noarch.rpm
                dnf clean all
                dnf makecache
                ```
                - dnf repolist 로 저장소 목록 확인
                ```
                dnf --enablerepo=elrepo-kernel install -y kernel-ml kernel-ml-devel
                ```
                - grubby --info=ALL | grep -i kernel 명령어로 설치된 커널 목록을 확인 후
                - grubby --set-default /boot/vmlinuz-새_커널_버전  을 입력

            * 그리고 이 에러가 나타난다면...
            ```
            [ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1
            ```
                - vi /etc/sysctl.conf 파일 수정
                - 다음 항목 추가
                ```
                net.ipv4.ip_forward = 1
                ```
                - 설정 적용
                ```
                sysctl -p
                ```
            * 또 이번엔 이 에러가 나왔다..
            ```
            [ERROR SystemVerification]: missing required cgroups: cpuset
            ```
                - vi /etc/default/grub 명령어로
                ```
                GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=1"
                ```
                - 해당 항목에 이 구문을 추가하고
                ```
                grub2-mkconfig -o /boot/grub2/grub.cfg
                ```
                입력 후 재부팅한다.
            * 설치는 되는데 다음 에러가 난다면..(pod 목록을 호출하거나 kubeadm token list 명령어 입력 시)
                ```            
                failed to list bootstrap tokens: Get "https://10.0.50.38:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=type%3Dbootstrap.kubernetes.io%2Ftoken": dial tcp 10.0.50.38:6443: connect: connection refused
                ```
                - sudo vi /etc/crictl.yaml 파일을 추가한다.
                ```
                runtime-endpoint: unix:///run/containerd/containerd.sock
                image-endpoint: unix:///run/containerd/containerd.sock
                timeout: 0
                debug: false
                ```
                - 아마 커널 업데이트때문에 발생한 현상같다. swapoff -a 이게 계속 먹는게 아니더라....
                - sudo vi /etc/fstab 이 파일에 swap 구문을 주석처리 또는 삭제한다.
                - 그 후 mount -a 입력 후 재부팅


        - kubaadm init - workerNode
            * kubeadm token list  명령어로 토큰 조회 후
            * openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 명령어로 hash조회 후
            * kubeadm join 마스터노드아이피:6443 --token 토큰 --discovery-token-ca-cert-hash sha256:해시

        - 초기화가 필요하다면
            * sudo kubeadm reset -f 를 입력
            * kubeadm init --apiserver-advertise-address=마스터노드IP --v=5
            * .kube 폴더에 config 파일 다시 이동해야함..
            ```
            unset KUBECONFIG
            export KUBECONFIG=/etc/kubernetes/admin.conf
            mkdir -p $HOME/.kube
            sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
            sudo chown $(id -u):$(id -g) $HOME/.kube/config
            ```
            * workerNode 오류
                - /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
                    * sudo modprobe br_netfilter
                - /proc/sys/net/ipv4/ip_forward contents are not set to 1
                    * echo '1' > /proc/sys/net/ipv4/ip_forward
        - 명령어 단축 관련 내용은 https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/ 참고

    - 확인한 에러...
        * k3s에서 k8s로 변경하고 masternode를 workerNode로 바꾸다보니 환경변수가 제거가 안되어있다. $KUBECONFIG 환경변수를 확인하고 제거해야한다면 제거해야한다.


- dns 내용 변경 필요시 /etc/db.lab 수정(DNS 서버로 구성시)
    * 난 ingress의 주소를 등록



- helm 설치
    ```
    curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
    chmod 700 get_helm.sh
    ./get_helm.sh
    ```
- Local Path Provisioner
    * 우선 storageclass부터 구성을 하자.. 안하면 PV 생성이 안된다...
    * Local Path Provisioner 설치
        - kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
        - kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

- keycloak
    * kubectl create -f https://raw.githubusercontent.com/keycloak/keycloak-quickstarts/refs/heads/main/kubernetes/keycloak.yaml -n keycloak
    * deploy에서 userid와 암호를 변경할 수 있다.
    * ingres & nip
        - wget https://raw.githubusercontent.com/keycloak/keycloak-quickstarts/refs/heads/main/kubernetes/keycloak-ingress.yaml
        - 위 구문은 필요시 수동으로 하자. 수동이 편할거다..
    * service에선 외부에서 접속할 포트를 port에 내부 포트를 targetport에 적는다.
    * 이후 설정
        - 출저 : https://wlsdn3004.tistory.com/62
        - 
    * jenkins
        - 플러그인 설치(keycloak, gitea)
        - realm 필요시 추가
        - Client 추가 일단 난 이렇게 했다.
            * Root URL : http://jenkins.lab
            * Home URL : http://jenkins.lab
            * Valid redirect URis : http://jenkins.lab/securityRealm/finishLogin
        - group 추가
        - user 추가
        - jenkins에서 security -> security realm -> keycloak Authentication Plugin 적용 및 client Json 적용    
    * 만약 DNS 서버가 없다..면...
        - DNS에 해당하는 URL을 hostalias로 추가한다.
            * jenkins
                - jenkins는 deploy가 없어 helm 의 values.yaml 편집 후 업그레이드한다.
                - helm get values jenkins -n jenkins > jenkins-values.yaml
                - helm upgrade jenkins jenkins/jenkins -n jenkins -f jenkins-values.yaml
                ```yaml
                controller:
                admin:
                    password: kaontech1@
                    username: admin
                hostAliases:
                - hostnames:
                    - keycloak.lab
                    ip: 10.103.246.172
                ```
            * gitea, harbor, argocd
                - 이거는 deploy가 구성이 되어있어 deploy 수정울 해서 해결을 했다.
                - keycloak.lab 의 서비스포트 수정이 필요해 서비스포트를 80으로 수정했다.


- gitea 설치
    * 이거도 일단 gitea namespace에 helm으로 설치를 해보긴 하자..
    * k create ns gitea
    * helm repo add gitea-charts https://dl.gitea.com/charts/
    * helm repo update
    * helm install gitea gitea-charts/gitea -n gitea
    * NodePort -> 31000포트로 만들거다.
        - k expose deploy gitea -n gitea --name=gitea-http --type=NodePort --port=3000
        - NodePort는 원하는 포트로 바꾸자
    * 아 또 pod가 안올라와....


- jenkins 설치
    * 이번엔 helm으로 설치해보자.
    ```
    helm repo add jenkins https://charts.jenkins.io
    helm repo update
    helm show values jenkins/jenkins > jenkins-values.yaml
    ```
    * 추출한 jenkins-values.yaml 에서 installPlugins 값을 false로 바꾸자.    
    ```
    helm install jenkins jenkins/jenkins -n jenkins -f jenkins-values.yaml
    ```
    * services의 type를 NodePort로 바꾸고 접속 확인을 했다.(32000 포트)
    * jenkins의 서비스 포트는 다음 포트를 포함해야 한다.
        - 8080(http)
        - 50000(tcp)

    * security설정 수정해서 로그인하게 처리
        - jenkins' own user database
        - 사용자의 가입허용
        - loggined-in users can do anything
    * https 설정?
        - openssl req -x509 -newkey rsa:4096 -keyout jenkins.key -out jenkins.crt -days 365 -nodes -subj "/CN=jenkins.lab"
        - kubectl create secret tls jenkins-tls --cert=jenkins.crt --key=jenkins.key -n jenkins
        - ingress에 다음 구문 추가
        ``` yaml        
        tls:
          - hosts:
            - jenkins.lab
            secretName: jenkins-tls
        ```
    * plugin 설치
        - 대상
            * gitea *
            * keycloak *
            * Docker Pipeline *
            * Maven Integration
            * Generic Webhook TriggerVersion
            * build pipeline
            * kubernetes
            * Kubernetes CLI
            * Kubernetes Credentials Provider
            * Pipeline Maven Plugin API
            * pipeline *
            * pipeline stage view
            * publish over ssh
            * Multibranch Scan Webhook Trigger
        - gitea 플러그인을 설치하자
        - jenkins 재부팅 필요 시 http://jenkins 주소/restart 로 직접 들어가서 restart 버튼을 누른다.
    * kubernetes 플러그인 설정
        - credential 에 kubernetes를 secretFile 형태로 추가한다. 파일은 .kube 폴더의 config 파일을 넣는다.
        - jenkins관리의 Clouds에 kubernetes 정보를 추가한다.
            * Kubernetes URL : 마스터노드IP:6443
            * Kubernetes Namespace : 구성할 namespace
            * Agent Docker Registry : docker.io
            * Credentials : 추가한 credential secretfile
    * pipeline
        - pipeline 생성을 한다.
        - 저장소
            * git 저장소 설정은 서비스 경로(http://gitea-http.gitea.svc.cluster.local:포트/이후경로) 로 한다.
            * credential은 거기에 맞게 지정해야한다.
            * branch 도 맟춘다. 일단 초기값은 */main 으로 지정했다.
        - 파이프라인
            * 일단 소스는 springboot maven으로 작업했다.
            * 이전의 샘플이 maven 빌드 후 나온 결과를 docker로 올리는 형태라 maven 컨테이너가 필요했으나 이번 파이프라인은 maven 빌드, docker 빌드를 다 한다.
            * 구섷한 컨테이너는 jnlp, docker, kustomize이다.
            * Jenkinsfile
                - 이번에는 별도의 DNS 구성이 없어 hostAliases 설정이 필요했다. 따라서 podTemplate 구성시 yaml형태로 구성했다.
                - kustomize 컨테이너를 구성했다. 이전엔 잘 안됐었던걸로 기억하는데 이번엔 된다,...
            * Dockerfile
                - 여기서 좀 해멨다.
                - 이 작업을 알려면 필요 구성을 알아야한다. 이렇게 해야 docker이미지가 실행이 된다.
                ```
                /app
                ├── pom.xml
                ├── src
                │   ├── main
                │   │   ├── java
                │   │   │   └── com/example/helloSpring (여기에 .java 파일이 있어야 함)
                │   │   ├── resources
                │   │   │   └── application.properties (환경 설정 파일)
                │   ├── test
                │   │   └── java (테스트 코드)
                ```
                - 추가로 테스트해본 내용이다.
                    * RUN unzip -l target/*.jar : jar 파일 내부 파일 목록 확인
                    * RUN find /app -name "*.jar" : 해당 폴더 내부 jar 파일 탐색
                    * RUN ls -la /app/src/main/java : 해당 폴더 파일 목록
    * Webhook
        - gitea와 jinkins의 Webhook 설정을 해주어야한다.
        - gitea
            * 서버 내부 설정이 필요하다. 설정파일이 pv로되어있어 서버 안으로 들어가서 수정하면 계속 적용이 되긴 한데 아무래도 helm으로 설치 때 적용이 필요할 듯 하다.
                - gitea 서버 내부 /data/gitea/conf/app.ini 파일을 수정한다.
                ```ini
                [webhook]
                ALLOWED_HOST_LIST = jenkins.jenkins.svc.cluster.local
                ```
            * 저장소 -> 설정 -> 웹훅 으로 들어가서 설정한다.
            * http://[jenkins 주소:서비스 포트(클러스터 내부 주소)]/generic-webhook-trigger/invoke?token=JENKINS계정 토큰
            * 테스트는 맨 아래 전달시험 버튼으로 해본다.
        - jenkins
            * 화면 우측 상단의 계정 버튼을 누르고 security -> API Token 항목에서 토큰을 생성한다.
            * jenkins의 파이프라인 설정에서 Generic Webhook Trigger 선택 후 Token를 집어넣던 credential 을 추가해서 넣던 하자..
            * gitea의 url로 이걸로 설정한다.










    * gitea ssh
        - 출처 : https://0andwild.tistory.com/28
        - jenkins credential에 private key를 등록해야한다.
        - ssh-keygen -t rsa -b 4096 -C "jenkins.lab" 뭐 이런식으로 인증서를 만들어보자..
        - .ssh 폴더에 만들어진 private key는 credential에, public key는 gitea에 등록하자.
        - 결국 못했다... 이전엔 파이프라인 안에 그냥 넣어버린 형태로 작업했으니 비슷하게 하면 될거같긴 한데 일단 http로 했으니 보류
        - 이걸 jenkins에 넣어야하나...
        ssh-keyscan -H gitea.lab >> ~/.ssh/known_hosts
        chmod 600 ~/.ssh/known_hosts






- metalLB
    * k create ns metallb-system
    * helm repo add metallb https://metallb.github.io/metallb
    * helm install metallb metallb/metallb -f metallb-install.yaml -n metallb-system
    * 여기서 metallb-install.yaml는 frr을 적용 안하는 설정값이다.
    ```yaml
    speaker:
    frr:
      enabled: false
    ```
    * 그 후 IPAddressPool 을 등록했다.
    ```yaml
    apiVersion: metallb.io/v1beta1
    kind: IPAddressPool
    metadata:
      name: metallb-ip-pool
      namespace: metallb-system
      spec:
        addresses:
        - 10.10.20.200-10.10.20.220
    ---
    apiVersion: metallb.io/v1beta1
    kind: L2Advertisement
    metadata:
      name: metallb-l2-ad
      namespace: metallb-system
      spec:
        ipAddressPools:
        - metallb-ip-pool
    ```
    * 잘못설치했을 때 다음 내용을 찾아 지우고 다시 설치해보라.
        - kubectl get crds | grep metallb
        - kubectl get validatingwebhookconfigurations | grep metallb

    * 기존 service를 clusterIP로 바꾸자
    ```yaml        
    spec:
      clusterIP: 10.110.140.90
      clusterIPs:
      - 10.110.140.90
    internalTrafficPolicy: Cluster
    ipFamilies:
      - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
        port: 80
        protocol: TCP
        targetPort: 8080
    selector:
        app.kubernetes.io/component: jenkins-controller
        app.kubernetes.io/instance: jenkins
    sessionAffinity: None
    type: ClusterIP
    ```
    * ingress를 구성하자.
    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
    name: jenkins-ingress
    namespace: jenkins
    spec:
      ingressClassName: nginx
      rules:
      - host: jenkins.lab
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: jenkins
                port:
                  number: 80
    ```
