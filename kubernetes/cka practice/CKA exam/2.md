1. 
controlplane ~ ➜  k create serviceaccount cicd-token -n app-team1 
serviceaccount/cicd-token created

controlplane ~ ➜  k create clusterrole deployment-clusterrole --resource=deployment,statefulset,daemonSet --verb=create
clusterrole.rbac.authorization.k8s.io/deployment-clusterrole created

controlplane ~ ➜  k create clusterrolebinding  deployment-clusterrolebinding --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token
clusterrolebinding.rbac.authorization.k8s.io/deployment-clusterrolebinding created





2. 
controlplane ~ ➜  k drain ek8s-node-0 --ignore-daemonsets





3. 
sudo apt update

kubectl drain mk8s-master-0 --ignore-daemonsets
ssh mk8s-master-0
sudo -i

sudo apt-cache madison kubeadm
sudo apt-mark unhold kubeadm
sudo apt-get update && sudo apt-get install -y kubeadm='1.31.1-*'
sudo apt-mark hold kubeadm
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.31.1

sudo apt-mark unhold kubelet kubectl
sudo apt-get update && sudo apt-get install -y kubelet='1.31.1-*' kubectl='1.31.1-*'
sudo apt-mark hold kubelet kubectl
sudo systemctl daemon-reload
sudo systemctl restart kubelet

exit
kubectl uncordon mk8s-master-0
k get nodes




4. 
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUINoo601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key snapshot save /var/lib/backup/etcd-snapshot.db.

>>>>systemstl stop etcd.service
etcdutl --data-dir /var/lib/backup/etcd-snapshot-previous.db snapshot restore snapshot.db
>>>>systemctl restart etcd.service




5. XXXXXXXXXXXXX
k label ns fubar product=fubar
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  ingress:
  - from:
    - namespaceselector:
        matchLabels:
          project: fubar
    ports:
    - protocol: TCP
      port: 9000
  policyTypes:
  - Ingress
```





6. 
```yaml
spec:
    containers:
    - name: nginx
      image: nginx
      ports:
      - containerPort: 80
      - name: http
```
k expose deploy front-end --name=front-end-svc --port=80 --type=nodeport




7. 
k scale deployment presentation --replicas=3



8. 
k run nginx-kusc00401 --image=nginx --dry-run=client -o yaml > test.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-kusc00401
  name: nginx-kusc00401
spec:
  containers:
  - image: nginx
    name: nginx-kusc00401
    resources: {}
  nodeSelector:
    disk: ssd
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```





9. 
k get nodes 
echo 2 > /opt/KUSC00402/kusc00402.txt



10. 
k run kucc8 --image=nginx --dry-run=client -o yaml > test2.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: kucc8
  name: kucc8
spec:
  containers:
  - image: nginx
    name: nginx
  - image: consul
    name: consul
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```





11. 
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-data
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadOnlyMany
  hostPath:
    path: "/srv/app-data"
```


12. 
k logs foo | grep "file-not-found" > /opt/KUTR00101/foo



13. 
```yaml
spec:
  containers:
  - image: sidecar
    name: busybox
    command: ["/bin/sh"]
    args: ["-c", "tail -n+1 -f /var/log/big-corp-app.log"]
    volumeMounts:
    - name: log
      mountPath: /log
```





14. 
k top pod -l name=overloaded-cpu --sort-by=cpu
echo overrr > /opt/KUTR00401/KUTR00401.txt 




15. XXXXXXXXXXXX
ssh wk8s-node-0
sudo -i
systemctl enable --now kubelet
systemctl restart kubelet
systemctl status kubelet
exit


16. 
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  resources:
    requests:
      storage: 10Mi
  storageClassName: csi-hostpath-sc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: pv
  volumes:
    - name: pv
      persistentVolumeClaim:
        claimName: pv-volume
```



17. XXX
XXXXXXXXXXk create ing pong -n ing-internal --rule=host/path=hello:5678 -> 이게 틀림 cmd로 하면 exact로 되네... prefix가 답임....
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  creationTimestamp: null
  name: pong
  namespace: ing-internal
spec:
  rules:
  - host: host
    http:
      paths:
      - backend:
          service:
            name: hello
            port:
              number: 5678
        path: /hello
        pathType: prefix
status:
  loadBalancer: {}
```

