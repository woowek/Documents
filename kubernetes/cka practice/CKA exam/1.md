1. Context -
You have been asked to create a new ClusterRole for a deployment pipeline and bind it to a specific ServiceAccount scoped to a specific namespace.<br>
Task -<br>
Create a new ClusterRole named deployment-clusterrole, which only allows to create the following resource types:<br>
✑ Deployment<br>
✑ Stateful Set<br>
✑ DaemonSet<br>
Create a new ServiceAccount named cicd-token in the existing namespace app-team1.<br>
Bind the new ClusterRole deployment-clusterrole to the new ServiceAccount cicd-token, limited to the namespace app-team1.<br>
    - k create clusterrole deployment-clusterrole --resource=deployment,statefunSet,daemonset --verb=create
    - k create serviceaccount cicd-token -n app-team1
    - k create clusterrolebinding deployment-clusterrole-binding --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token 



2. Task -<br>XXXXXXXXXX
Set the node named ek8s-node-0 as unavailable and reschedule all the pods running on it.
    - k drain ek8s-node-0 --ignore-daemonsets
    - k drain ek8s-node-0 --ignore-daemonsets --delete-emptydir-data



3. Task -<br>XXXXXXXXXX
Given an existing Kubernetes cluster running version 1.22.1, upgrade all of the Kubernetes control plane and node components on the master node only to version 1.22.2.<br>
Be sure to drain the master node before upgrading it and uncordon it after the upgrade.<br>
You are also expected to upgrade kubelet and kubectl on the master node.
    - k drain mk8s-master-0 --ignore-daemonsets
    - ssh mk8s-master-0
    - sudo apt update
    - sudo apt-mark unhold kubelet kubectl && \
    - sudo apt-get update && sudo apt-get install -y kubeadm='1.22.2-00' kubelet='1.22.2-00' kubectl='1.22.2-00' && \
    - sudo apt-mark hold kubelet kubectl
    - kubeadm upgrade plan
    - sudo kubeadm upgrade apply v1.22.2
    - sudo systemctl daemon-reload
    - sudo systemctl restart kubelet
    - exit
    - k uncordon mk8s-master-0
    - k get nodes



4. Task -<br>XXXXXXXX
First, create a snapshot of the existing etcd instance running at https://127.0.0.1:2379, saving the snapshot to /var/lib/backup/etcd-snapshot.db.<br>
Next, restore an existing, previous snapshot located at /var/lib/backup/etcd-snapshot-previous.db.
    - ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUIN00601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key snapshot save /var/lib/backup/etcd-snapshot.db
    - sudo systemctl stop etcdutl.service
    - etcdutl --data-dir /var/lib/backup/etcd-snapshot-previous.db snapshot restore snapshot.db
    - sudo systemctl restart etcdutl.service



5. Task -<br>XXXX
Create a new NetworkPolicy named allow-port-from-namespace in the existing namespace fubar.<br>
Ensure that the new NetworkPolicy allows Pods in namespace internal to connect to port 9000 of Pods in namespace fubar.<br>
Further ensure that the new NetworkPolicy:<br>
✑ does not allow access to Pods, which don't listen on port 9000<br>
✑ does not allow access from Pods, which are not in namespace internal<br>
    - k label ns fubar project=fubar
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: fubar
spec:
  podSelector: {}
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          project: fubar
    ports:
    - protocol: TCP
      port: 9000
  policyTypes:
  - Ingress
```




6. Task -<br>XXX
Reconfigure the existing deployment front-end and add a port specification named http exposing port 80/tcp of the existing container nginx.<br>
Create a new service named front-end-svc exposing the container port http.<br>
Configure the new service to also expose the individual Pods via a NodePort on the nodes on which they are scheduled.<br>
```yaml
    ports:
    - containerPort: 80
```
    - k expose deploy front-end --name=front-end-svc --port=80 --type=NodePort --protocol=TCP





7. Task -<br>
Scale the deployment presentation to 3 pods.
    - k scale deployment presentation -replicas=3



8. Task -<br>
Schedule a pod as follows:<br>
✑ Name: nginx-kusc00401<br>
✑ Image: nginx<br>
✑ Node selector: disk=ssd<br>
    - k run nginx-kusc00401 --image=nginx --dry-run=client -o yaml > nginx.yaml
```yaml
spec:
  nodeSelector:
    disk: ssd
```






9. Task -<br>XXXXX
Check to see how many nodes are ready (not including nodes tainted NoSchedule) and write the number to /opt/KUSC00402/kusc00402.txt.
    - 왜 2지???????????? 내가 보기엔 일일이 taint를 봐야할거같은데..




10. Task -<br>
Schedule a Pod as follows:<br>
✑ Name: kucc8<br>
✑ App Containers: 2<br>
✑ Container Name/Images:<br>
- nginx<br>
- consul
```yaml
spec:
  containers:
  - name: nginx
    image: nginx
  - name: consul
    image: consul
```




11. Task -<br>
Create a persistent volume with name app-data, of capacity 2Gi and access mode ReadOnlyMany. The type of volume is hostPath and its location is /srv/app-data.
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-data
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadOnlyMany
  hostPath:
    path: "/srv/app-data"
```





12. Task -<br>XXXXXXXXXXXXXX
Monitor the logs of pod foo and:<br>
✑ Extract log lines corresponding to error file-not-found<br>
✑ Write them to /opt/KUTR00101/foo
    - k logs foo | grep file-not-found > /opt/KUTR00101/foo



13. Context -<br>XXXXXXXXXXXXXX
An existing Pod needs to be integrated into the Kubernetes built-in logging architecture (e.g. kubectl logs). Adding a streaming sidecar container is a good and common way to accomplish this requirement.<br>
Task -<br>
Add a sidecar container named sidecar, using the busybox image, to the existing Pod big-corp-app. The new sidecar container has to run the following command:<br>
Use a Volume, mounted at /var/log, to make the log file big-corp-app.log available to the sidecar container.
    - k edit pod big-corp-app
```yaml
spec:
  containers:
  - name: sidecar
    image: busybox
    command: ["/bin/sh"]
    args: ["-c ", "tail -n+1 -f /var/log/big-corp-app.log"]
    volumeMounts:
    - mountPath: /var/log
      name: logs
```





14. Task -<br>XXXXXXXXXXXXXX
From the pod label name=overloaded-cpu, find pods running high CPU workloads and write the name of the pod consuming most CPU to the file /opt/KUTR00401/KUTR00401.txt (which already exists).
    - k top pods
    - k top pods -l name=overloaded-cpu --sort-by=CPU
    - echo overloded-cpu-sdf > /opt/KUTR00401/KUTR00401.txt





15. Task -<br>
A Kubernetes worker node, named wk8s-node-0 is in state NotReady.<br>
Investigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made permanent.
    - 문제만으론 할수있는게 없다...
    - ssh wk8s-node-0
    - systemctl restart kubelet.service
    - exit



16. Task -<br>XXXXXXXXXX
Create a new PersistentVolumeClaim:
✑ Name: pv-volume
✑ Class: csi-hostpath-sc
✑ Capacity: 10Mi
Create a new Pod which mounts the PersistentVolumeClaim as a volume:
✑ Name: web-server
✑ Image: nginx
✑ Mount path: /usr/share/nginx/html
Configure the new Pod to have ReadWriteOnce access on the volume.
Finally, using kubectl edit or kubectl patch expand the PersistentVolumeClaim to a capacity of 70Mi and record that change.
    - 
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Mi
  storageClassName: csi-hostpath-sc
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: pv-volume
  volumes:
    - name: pv-volume
      persistentVolumeClaim:
        claimName: pv-volume
```




17. Task -<br>
Create a new nginx Ingress resource as follows:<br>
✑ Name: pong<br>
✑ Namespace: ing-internal<br>
✑ Exposing service hello on path /hello using service port 5678<br>
    - k create ing pong -n ing-internal --rule=host/path=hello:5678